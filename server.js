require('dotenv').config();
const express = require('express');
const multer = require('multer');
const sdk = require('microsoft-cognitiveservices-speech-sdk');
const { AzureOpenAI } = require('openai');
const cors = require('cors');
const fs = require('fs');
const path = require('path');
const ffmpeg = require('fluent-ffmpeg');
const wav = require('wav');
const graphTools = require('./graph-tools');
const ttsService = require('./tts-service');

// Set ffmpeg path based on environment
let ffmpegPath;
if (process.env.NODE_ENV === 'production' || process.env.DOCKER_ENV) {
  // In Docker and Azure Linux, ffmpeg is in PATH
  ffmpeg.setFfmpegPath('ffmpeg');
  console.log('âœ“ Using system FFmpeg (Docker/Linux)');
} else {
  // For local Windows development
  ffmpegPath = process.env.FFMPEG_PATH || 'C:\\ffmpeg\\bin\\ffmpeg.exe';
  ffmpeg.setFfmpegPath(ffmpegPath);
  console.log(`âœ“ Using custom FFmpeg path: ${ffmpegPath}`);
}

// Import authentication router
const { router: authRouter, userTokenStore } = require('./auth');

const app = express();
const upload = multer({
  storage: multer.memoryStorage(),
  limits: { fileSize: 50 * 1024 * 1024 } // 50MB limit
});

app.use(cors());
app.use(express.json());
app.use(express.static('public'));

// Mount auth routes
app.use('/auth', authRouter);

// Get config endpoint
app.get('/api/config', (req, res) => {
  res.json({
    configured: !!(process.env.AZURE_SPEECH_KEY && process.env.AZURE_OPENAI_KEY)
  });
});

// Get available voice accents/languages
app.get('/api/voices', (req, res) => {
  const voices = ttsService.getAvailableVoices();
  const formattedVoices = Object.entries(voices).map(([key, value]) => ({
    id: key,
    name: value.displayName,
    voiceName: value.name,
    language: value.language
  }));
  
  res.json({
    voices: formattedVoices,
    default: 'american'
  });
});

// Clear conversation history endpoint
app.post('/api/clear-session', express.json(), (req, res) => {
  const sessionId = req.body.sessionId;
  if (sessionId && conversationSessions.has(sessionId)) {
    conversationSessions.delete(sessionId);
    console.log(`âœ“ Session ${sessionId} cleared`);
  }
  res.json({ success: true });
});

// Debug endpoint - view conversation history
app.get('/api/debug/sessions', (req, res) => {
  const sessions = {};
  for (const [sessionId, history] of conversationSessions.entries()) {
    sessions[sessionId] = {
      messageCount: history.length,
      messages: history
    };
  }
  res.json(sessions);
});

// Get user profile info (name, email, etc)
app.get('/api/user-profile', async (req, res) => {
  try {
    const sessionId = req.query.sessionId;
    if (!sessionId || !userTokenStore.has(sessionId)) {
      return res.status(401).json({ error: 'No valid session' });
    }

    const token = userTokenStore.get(sessionId);
    const profileInfo = await graphTools.getSenderProfile(token);
    
    res.json({
      displayName: profileInfo.displayName,
      email: profileInfo.email,
      firstName: profileInfo.displayName.split(' ')[0]
    });
  } catch (err) {
    console.error('Error fetching user profile:', err);
    res.status(500).json({ error: 'Failed to fetch user profile' });
  }
});

// Get user profile photo
app.get('/api/user-photo', async (req, res) => {
  try {
    const sessionId = req.query.sessionId;
    console.log('ğŸ“· Photo request for session:', sessionId);
    
    if (!sessionId || !userTokenStore.has(sessionId)) {
      console.warn('âŒ Invalid session for photo request');
      return res.status(401).json({ error: 'No valid session' });
    }

    const token = userTokenStore.get(sessionId);
    console.log('ğŸ“· Fetching photo with token...');
    
    const photoBuffer = await graphTools.getUserProfilePhoto(token);
    console.log('ğŸ“· Photo buffer returned, type:', typeof photoBuffer, 'length:', photoBuffer ? photoBuffer.length : 'null');
    
    if (!photoBuffer) {
      console.warn('âš ï¸ No photo buffer returned');
      return res.status(404).json({ error: 'No profile photo found' });
    }

    if (Buffer.isBuffer(photoBuffer)) {
      console.log('âœ… Photo is a proper Buffer, size:', photoBuffer.length);
    } else {
      console.warn('âš ï¸ Photo is not a Buffer, type:', typeof photoBuffer);
    }

    res.set('Content-Type', 'image/jpeg');
    res.send(photoBuffer);
  } catch (err) {
    console.error('âŒ Error fetching user photo:', err);
    res.status(500).json({ error: 'Failed to fetch user photo' });
  }
});

// Store conversation history per session (in production, use database)
const conversationSessions = new Map();

// Endpoint to process voice interaction
app.post('/api/process-voice', upload.single('audio'), async (req, res) => {
  const tempWebm = path.join(__dirname, `temp_${Date.now()}.webm`);
  const tempWav = path.join(__dirname, `temp_${Date.now()}.wav`);

  try {
    console.log('\n=== Voice Processing Started ===');

    if (!req.file) {
      throw new Error('No audio file uploaded');
    }

    // Get or create session ID
    const sessionId = req.body.sessionId || 'default';
    const accent = req.body.accent || 'american';
    const language = req.body.language || 'en-US';

    if (!conversationSessions.has(sessionId)) {
      conversationSessions.set(sessionId, []);
    }

    const audioBuffer = req.file.buffer;
    console.log('âœ“ Audio received:', {
      size: audioBuffer.length,
      type: req.file.mimetype,
      sessionId: sessionId,
      accent: accent,
      language: language
    });

    // Validate accent
    if (!ttsService.isValidAccent(accent)) {
      throw new Error(`Invalid accent: ${accent}. Valid options: american, british, japanese`);
    }

    // Save WebM file
    fs.writeFileSync(tempWebm, audioBuffer);
    console.log('âœ“ Audio saved to temp file:', tempWebm);

    // Convert WebM to WAV using ffmpeg
    console.log('ğŸ”„ Converting audio to WAV format...');
    await convertToWav(tempWebm, tempWav);
    console.log('âœ“ Audio converted to WAV:', tempWav);

    // Read WAV file
    const wavBuffer = fs.readFileSync(tempWav);
    console.log('âœ“ WAV file loaded, size:', wavBuffer.length);

    // Step 1: Speech-to-Text
    console.log('ğŸ¤ Starting speech-to-text...');
    const transcript = await speechToText(wavBuffer);
    console.log('âœ“ Transcript:', transcript);

    if (!transcript || transcript.trim() === '') {
      throw new Error('No speech detected in the audio. Please speak louder and try again.');
    }

    // Step 2: Query Azure OpenAI Agent
    console.log('ğŸ¤– Querying AI agent...');
    const conversationHistory = conversationSessions.get(sessionId);
    const userToken = userTokenStore.get(sessionId);

    if (!userToken) {
      console.warn('âš ï¸  No user token found for session:', sessionId);
    }

    const agentResponse = await queryAgent(transcript, conversationHistory, sessionId, userToken);
    console.log('âœ“ Agent Response:', agentResponse);

    // Step 3: Text-to-Speech with selected accent
    console.log(`ğŸ”Š Generating speech with ${ttsService.getVoiceInfo(accent).displayName}...`);
    const audioData = await ttsService.synthesizeText(agentResponse, accent);
    console.log('âœ“ Audio generated, size:', audioData.length);

    // Clean up temp files
    [tempWebm, tempWav].forEach(file => {
      if (fs.existsSync(file)) {
        fs.unlinkSync(file);
      }
    });

    res.json({
      transcript,
      agentResponse,
      audioData: audioData.toString('base64'),
      sessionId: sessionId
    });
  } catch (error) {
    console.error('âŒ Error:', error.message);

    // Clean up temp files on error
    [tempWebm, tempWav].forEach(file => {
      if (fs.existsSync(file)) {
        try { fs.unlinkSync(file); } catch (e) { }
      }
    });

    res.status(500).json({
      error: error.message || 'Unknown error occurred'
    });
  }
});

// Convert audio to WAV format using ffmpeg
function convertToWav(inputPath, outputPath) {
  return new Promise((resolve, reject) => {
    ffmpeg(inputPath)
      .toFormat('wav')
      .audioFrequency(16000)
      .audioChannels(1)
      .audioCodec('pcm_s16le')
      .on('end', () => {
        console.log('  âœ“ FFmpeg conversion completed');
        resolve();
      })
      .on('error', (err) => {
        console.error('  âŒ FFmpeg error:', err.message);
        reject(new Error('Audio conversion failed: ' + err.message));
      })
      .save(outputPath);
  });
}

// Speech-to-Text using Azure Speech Services (Single-shot recognition)
async function speechToText(wavBuffer) {
  return new Promise((resolve, reject) => {
    let recognizer = null;

    try {
      const speechKey = process.env.AZURE_SPEECH_KEY;
      const speechRegion = process.env.AZURE_SPEECH_REGION;

      if (!speechKey || !speechRegion) {
        reject(new Error('Azure Speech credentials not configured in .env file'));
        return;
      }

      console.log('  â†’ Initializing Azure Speech SDK...');
      const speechConfig = sdk.SpeechConfig.fromSubscription(speechKey, speechRegion);
      speechConfig.speechRecognitionLanguage = 'en-US';

      console.log('  â†’ Creating audio config from WAV buffer...');

      // Create audio stream from WAV buffer
      const pushStream = sdk.AudioInputStream.createPushStream();

      // Skip WAV header (first 44 bytes) and push the raw PCM data
      const pcmData = wavBuffer.slice(44);
      pushStream.write(pcmData);
      pushStream.close();

      const audioConfig = sdk.AudioConfig.fromStreamInput(pushStream);
      recognizer = new sdk.SpeechRecognizer(speechConfig, audioConfig);

      console.log('  â†’ Starting single-shot recognition...');

      // Single-shot recognition - recognizes once and returns
      recognizer.recognizeOnceAsync(
        (result) => {
          recognizer.close();

          if (result.reason === sdk.ResultReason.RecognizedSpeech) {
            console.log(`  âœ“ Recognized: "${result.text}"`);
            resolve(result.text);
          } else if (result.reason === sdk.ResultReason.NoMatch) {
            console.log('  âš  No speech could be recognized');
            reject(new Error('No speech was detected. Please speak clearly and try again.'));
          } else if (result.reason === sdk.ResultReason.Canceled) {
            const cancellation = sdk.CancellationDetails.fromResult(result);
            console.error(`  âŒ Canceled: ${cancellation.reason}`);
            if (cancellation.reason === sdk.CancellationReason.Error) {
              reject(new Error(`Speech recognition error: ${cancellation.errorDetails}`));
            } else {
              reject(new Error('Speech recognition was canceled.'));
            }
          }
        },
        (err) => {
          recognizer.close();
          console.error('  âŒ Recognition error:', err);
          reject(new Error('Speech recognition failed: ' + err));
        }
      );

    } catch (error) {
      console.error('  âŒ Exception:', error.message);
      if (recognizer) recognizer.close();
      reject(new Error('Speech recognition initialization failed: ' + error.message));
    }
  });
}

// Query Azure OpenAI Agent
// Replace the queryAgent function in your server.js with this updated version
// Query Azure OpenAI Agent
// Replace the queryAgent function in your server.js with this updated version

async function queryAgent(text, conversationHistory = [], sessionId = 'default', userToken = null) {
  try {
    const endpoint = process.env.AZURE_OPENAI_ENDPOINT;
    const apiKey = process.env.AZURE_OPENAI_KEY;
    const deployment = process.env.AZURE_OPENAI_DEPLOYMENT || 'gpt-4o-mini';

    if (!endpoint || !apiKey) {
      throw new Error('Azure OpenAI credentials not configured in .env file');
    }

    console.log('  â†’ Sending request to Azure OpenAI...');
    console.log('  â†’ Conversation history length:', conversationHistory.length);
    console.log('  â†’ User token available:', !!userToken);

    const client = new AzureOpenAI({
      endpoint: endpoint,
      apiKey: apiKey,
      apiVersion: '2024-08-01-preview',
      deployment: deployment
    });

    // Load agent tools
    const { tools, executeTool } = require('./agent-tools');

    // Get current date for context
    const now = new Date();
    const currentDate = now.toISOString().split('T')[0];
    const currentTime = now.toTimeString().split(' ')[0];

    // Build messages array with conversation history
    const messages = [
      {
        role: 'system',
        content: `You are a helpful AI voice assistant with access to Microsoft 365 services.

CURRENT DATE & TIME: ${currentDate} ${currentTime}
Today is: ${now.toLocaleDateString('en-US', { weekday: 'long', year: 'numeric', month: 'long', day: 'numeric' })}

================================================================================
ğŸ“… MEETING RULE â€” DEFAULT TO TEAMS MEETING
================================================================================
When the user schedules ANY meeting, ALWAYS set:
isTeamsMeeting = true

UNLESS the user clearly says:
- "offline meeting"
- "in person"
- "physical room"
- "not on teams"
- "not online"

So by default, meetings must be created as Teams meetings.

================================================================================
ğŸ“¤ MEETING LINK SHARING RULE
================================================================================
After creating a Teams meeting, tell the user:
"The Teams meeting link will be shared automatically with all participants."

================================================================================
ğŸ“§ EMAIL SENDING - PROFESSIONAL FORMATTING:
================================================================================
When user wants to send email, ALWAYS use the send_email tool.

Examples:
- "send mail to jatin raj about app issue" â†’ send_email(recipient_name="jatin raj", subject="Application Issue", body="...")
- "email vansh about meeting" â†’ send_email(recipient_name="vansh", subject="Meeting", body="...")

The system will automatically:
âœ“ Add professional greeting with recipient's name
âœ“ Format body with HTML styling
âœ“ Include your signature with name, title, contact
âœ“ Add footer with date

YOU MUST:
âœ“ Call send_email tool for ANY email request
âœ“ Use recipient's name (not email address)
âœ“ Create clear subject
âœ“ Write natural body text

âš ï¸ CONTACT NOT FOUND HANDLING:
If send_email, send_teams_message, or search_contact_email returns notFound=true or found=false:
1. Tell the user the contact was not found
2. Ask them to verify the spelling of the name
3. OR ask them to provide the email address directly
Example response: "I couldn't find anyone named 'aman' in the directory. Could you check the spelling or provide their email address?"

================================================================================
ğŸ“… MEETING SCHEDULING - WITH MULTIPLE ATTENDEES:
================================================================================
When user wants to schedule meeting, ALWAYS use the create_calendar_event tool.

DATE/TIME CALCULATIONS:
- Today: ${currentDate}
- "3 PM today" â†’ ${currentDate}T15:00:00
- "tomorrow 2 PM" â†’ ${new Date(now.getTime() + 86400000).toISOString().split('T')[0]}T14:00:00
- "10 AM" â†’ current_date + T10:00:00
- "2:30 PM" â†’ current_date + T14:30:00

DURATION DEFAULTS:
- "30 min" â†’ add 30 minutes to start time
- "1 hour" â†’ add 60 minutes to start time
- No duration specified â†’ default 1 hour

ğŸ“Œ DEFAULT TEAM MEETING (MOST IMPORTANT UPDATE)
SET:
isTeamsMeeting = true
UNLESS user explicitly denies.

Examples:
User: "schedule meet with jatin raj 3 PM today for 30 min"
YOU MUST CALL: create_calendar_event(
  subject="Meeting with Jatin Raj",
  start="${currentDate}T15:00:00",
  end="${currentDate}T15:30:00",
  attendeeNames=["jatin raj"],
  isTeamsMeeting=true
)

User: "set up teams call with john and sarah tomorrow 10 AM"
YOU MUST CALL: create_calendar_event(
  subject="Teams Meeting",
  start="${new Date(now.getTime() + 86400000).toISOString().split('T')[0]}T10:00:00",
  end="${new Date(now.getTime() + 86400000).toISOString().split('T')[0]}T11:00:00",
  attendeeNames=["john", "sarah"],
  isTeamsMeeting=true
)

================================================================================
ğŸ—‘ï¸ DELETION FEATURES:
================================================================================
DELETE EMAIL:
- "delete the email I just sent" â†’ delete_sent_email()
- "delete the email about meeting" â†’ delete_sent_email(subject="meeting")
- "delete the email to john" â†’ First search_contact_email("john"), then delete_sent_email(recipient_email="john@...")

DELETE CALENDAR EVENT:
- "delete the meeting with raj" â†’ delete_calendar_event(subject="raj")
- "cancel the standup meeting" â†’ delete_calendar_event(subject="standup")

DELETE TEAMS MESSAGE:
- "delete the teams message I just sent" â†’ delete_teams_message(chat_id=..., message_id=...)

================================================================================
ğŸš¨ CRITICAL: YOU MUST USE TOOLS!
================================================================================
- For email requests â†’ CALL send_email tool
- For meeting requests â†’ CALL create_calendar_event tool
- For calendar questions â†’ CALL get_calendar_events tool
- For email questions â†’ CALL get_recent_emails tool
- For deleting emails â†’ CALL delete_sent_email tool
- For deleting meetings â†’ CALL delete_calendar_event tool
- For Teams messages â†’ CALL send_teams_message or delete_teams_message tool

DO NOT just respond with text. ALWAYS call the appropriate tool when user requests an action.

Keep voice responses short (1-2 sentences) after tool execution.
`
      }
    ];

    // Add conversation history
    conversationHistory.forEach(msg => {
      messages.push(msg);
    });

    // Add current user message
    messages.push({
      role: 'user',
      content: text
    });

    console.log('  â†’ Tools available:', tools.length);
    console.log('  â†’ Tool names:', tools.map(t => t.function.name).join(', '));

    // First call - AI decides if it needs to use tools
    let result = await client.chat.completions.create({
      model: deployment,
      messages: messages,
      tools: tools,
      tool_choice: 'auto', // Let AI decide when to use tools
      max_tokens: 500,
      temperature: 0.7
    });

    let responseMessage = result.choices[0].message;
    console.log('  â†’ AI response:', {
      hasToolCalls: !!responseMessage.tool_calls,
      toolCallCount: responseMessage.tool_calls?.length || 0,
      content: responseMessage.content || '(no content)'
    });

    // Check if AI wants to use tools
    if (responseMessage.tool_calls && responseMessage.tool_calls.length > 0) {
      console.log('  â†’ AI requesting tool execution:', responseMessage.tool_calls.map(tc => tc.function.name).join(', '));

      // Add AI's response to messages
      messages.push(responseMessage);

      // Execute all requested tools
      for (const toolCall of responseMessage.tool_calls) {
        const functionName = toolCall.function.name;
        const functionArgs = JSON.parse(toolCall.function.arguments);

        console.log(`  â†’ Executing ${functionName} with args:`, JSON.stringify(functionArgs, null, 2));

        try {
          // âœ… FIXED: Pass userToken as third parameter to executeTool
          const toolResult = await executeTool(functionName, functionArgs, userToken);
          console.log(`  âœ“ ${functionName} completed:`, toolResult);

          // Add tool result to messages
          messages.push({
            role: 'tool',
            tool_call_id: toolCall.id,
            content: JSON.stringify(toolResult)
          });
        } catch (error) {
          console.error(`  âœ— ${functionName} failed:`, error.message);
          messages.push({
            role: 'tool',
            tool_call_id: toolCall.id,
            content: JSON.stringify({ error: error.message })
          });
        }
      }

      // Second call - AI formulates final response with tool results
      console.log('  â†’ Getting final response from AI...');
      result = await client.chat.completions.create({
        model: deployment,
        messages: messages,
        max_tokens: 300,
        temperature: 0.7
      });

      responseMessage = result.choices[0].message;
      console.log('  âœ“ Final response:', responseMessage.content);
    } else {
      console.log('  â„¹ No tools were called by AI');
    }

    // Update conversation history
    conversationHistory.push({
      role: 'user',
      content: text
    });
    conversationHistory.push({
      role: 'assistant',
      content: responseMessage.content
    });

    // Keep only last 10 exchanges (20 messages)
    if (conversationHistory.length > 20) {
      conversationHistory.splice(0, conversationHistory.length - 20);
    }

    console.log('  âœ“ Received response from AI');
    return responseMessage.content;
  } catch (error) {
    console.error('  âœ— OpenAI error:', error.message);
    if (error.response) {
      console.error('  âœ— Error details:', error.response.data);
    }
    throw new Error('Failed to get AI response: ' + error.message);
  }
}

// Text-to-Speech using Azure Speech Services
async function textToSpeech(text) {
  // Deprecated: Use ttsService.synthesizeText() instead
  // This function is kept for backwards compatibility only
  console.warn('âš ï¸  textToSpeech() is deprecated, use ttsService.synthesizeText() instead');
  return ttsService.synthesizeText(text, 'american');
}

// Endpoint to process text messages
app.post('/api/text-message', express.json(), async (req, res) => {
  try {
    const { text, sessionId, language, accent } = req.body;

    if (!text || !text.trim()) {
      return res.status(400).json({ error: 'Text message is required' });
    }

    if (!sessionId) {
      return res.status(400).json({ error: 'Session ID is required' });
    }

    const selectedAccent = accent || 'american';
    
    // Validate accent
    if (!ttsService.isValidAccent(selectedAccent)) {
      return res.status(400).json({ error: `Invalid accent: ${selectedAccent}` });
    }

    console.log('\n=== Text Message Received ===');
    console.log(`âœ“ Message: "${text}"`);
    console.log(`âœ“ Session ID: ${sessionId}`);
    console.log(`âœ“ Accent: ${selectedAccent}`);

    // Retrieve user token from session store
    const userToken = userTokenStore.get(sessionId);
    if (!userToken) {
      return res.status(401).json({ error: 'Invalid or expired session' });
    }

    // Query the AI agent with the text message
    const response = await queryAgent(text, conversationSessions.get(sessionId) || [], sessionId, userToken);

    // Get or create conversation history for this session
    if (!conversationSessions.has(sessionId)) {
      conversationSessions.set(sessionId, []);
    }
    const conversationHistory = conversationSessions.get(sessionId);

    // Add to conversation history
    conversationHistory.push({
      role: 'user',
      content: text
    });
    conversationHistory.push({
      role: 'assistant',
      content: response
    });

    // Keep only last 20 messages
    if (conversationHistory.length > 20) {
      conversationHistory.splice(0, conversationHistory.length - 20);
    }

    console.log('âœ“ Response generated successfully');

    res.json({
      success: true,
      response: response,
      sessionId: sessionId
    });

  } catch (error) {
    console.error('âŒ Error processing text message:', error.message);
    res.status(500).json({
      error: error.message || 'Failed to process text message'
    });
  }
});

const PORT = process.env.PORT || 3000;
app.listen(PORT, () => {
  console.log(`
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘   Azure Voice AI Agent Server Running     â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  
  ğŸŒ URL: http://localhost:${PORT}
  
  Configuration Status:
  ${process.env.AZURE_SPEECH_KEY ? 'âœ“' : 'âœ—'} Azure Speech Service ${process.env.AZURE_SPEECH_REGION ? `(${process.env.AZURE_SPEECH_REGION})` : ''}
  ${process.env.AZURE_OPENAI_KEY ? 'âœ“' : 'âœ—'} Azure OpenAI ${process.env.AZURE_OPENAI_DEPLOYMENT ? `(${process.env.AZURE_OPENAI_DEPLOYMENT})` : ''}
  
  ${!process.env.AZURE_SPEECH_KEY || !process.env.AZURE_OPENAI_KEY ?
      'âš ï¸  Please configure your .env file with Azure credentials\n' : 'âœ“ All services configured - Ready to use!\n'}
  `);
});